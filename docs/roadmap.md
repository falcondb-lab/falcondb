# FalconDB Roadmap & Milestone Acceptance Criteria

---

## v0.1.0 ‚Äî M1: Stable OLTP Foundation ‚úÖ

**Status**: Released

### Deliverables

| Feature | Acceptance Criteria | Status |
|---------|-------------------|--------|
| MVCC storage engine | VersionChain, MemTable, DashMap indexes | ‚úÖ |
| WAL persistence | Segment rotation, CRC32 checksums, group commit, fdatasync | ‚úÖ |
| Transaction manager | LocalTxn (fast-path OCC+SI), GlobalTxn (slow-path 2PC) | ‚úÖ |
| SQL frontend | DDL/DML/SELECT, CTEs, window functions, subqueries, set ops | ‚úÖ |
| PG wire protocol | Simple + extended query, COPY, auth (Trust/MD5/SCRAM) | ‚úÖ |
| In-process replication | WAL shipping via ShardReplicaGroup, catch-up, promote | ‚úÖ |
| Failover | 5-step fencing protocol, promote semantics | ‚úÖ |
| MVCC GC | Background GcRunner, safepoint, replica-safe | ‚úÖ |
| Benchmarks | YCSB harness, fast-path comparison, scale-out, failover | ‚úÖ |
| Observability | SHOW falcon.*, Prometheus metrics, structured tracing | ‚úÖ |

### Test Coverage

- 1,081 passing tests across 13 crates
- Integration tests for DDL/DML/SELECT end-to-end
- Failover exercise (create ‚Üí replicate ‚Üí fence ‚Üí promote ‚Üí verify)

### Verification Commands

```bash
cargo test -p falcon_storage          # 226 tests (MVCC, WAL, GC, indexes)
cargo test -p falcon_cluster           # 247 tests (replication, failover, scatter/gather)
cargo test -p falcon_server            # 208 tests (SQL e2e, SHOW, error paths)
cargo test -p falcon_txn               # 53 tests (txn lifecycle, OCC, stats)
cargo test -p falcon_sql_frontend      # 141 tests (parsing, binding, analysis)
cargo test -p falcon_protocol_pg       # 147 tests (PG wire, auth, SHOW commands)
cargo test -p falcon_bench -- --help   # benchmark harness
```

---

## v0.2.0 ‚Äî M2: gRPC WAL Streaming & Multi-Node üîÑ

**Status**: In progress

### Deliverables

| Feature | Acceptance Criteria | Status |
|---------|-------------------|--------|
| gRPC WAL streaming | tonic server/client, `SubscribeWal` server-streaming RPC | ‚úÖ |
| Replica runner | `ReplicaRunner` with exponential backoff, auto-reconnect | ‚úÖ |
| Checkpoint streaming | `GetCheckpoint` RPC for new replica bootstrap | ‚úÖ |
| Ack tracking | Replica reports `applied_lsn`, primary tracks lag | ‚úÖ |
| Multi-node CLI | `--role primary/replica`, `--grpc-addr`, `--primary-endpoint` | ‚úÖ |
| Durability policies | `local-fsync`, `quorum-ack`, `all-ack` config options | ‚úÖ |
| WAL backpressure | Admission control on WAL backlog + replication lag | ‚úÖ |
| Config schema | `--print-default-config`, full TOML schema documented | ‚úÖ |
| Replication log capacity | `max_capacity` eviction, bounded memory growth | ‚úÖ |
| Replica lag metric | `lag_lsn` in ReplicaRunnerMetrics | ‚úÖ |

### Acceptance Gates (must pass before M2 release)

1. **Two-node smoke test**: Primary + Replica start via CLI, data written on
   primary is readable on replica within 2 seconds.
2. **Reconnect resilience**: Kill and restart replica; it reconnects and
   catches up without data loss.
3. **Checkpoint bootstrap**: New replica joins via `GetCheckpoint` RPC,
   receives full snapshot, then switches to WAL streaming.
4. **Failover under load**: pgbench running on primary, promote replica,
   new primary serves reads+writes within 5 seconds.
5. **CI gate**: `scripts/ci_failover_gate.sh` passes with 0 failures.
6. **RPO validation**: Under `quorum-ack`, kill primary after commit ‚Äî verify
   promoted replica has all committed data.

### Remaining Work

| Task | Priority | Estimate |
|------|----------|----------|
| End-to-end multi-node integration test (psql-based) | P0 | 2 days |
| `quorum-ack` commit path wiring (wait for replica acks) | P0 | 3 days |
| Shard-aware replication (multi-shard per node) | P1 | 3 days |
| Proto file rename: `falcon_replication.proto` | P2 | Done ‚úÖ |
| Documentation: protocol compatibility matrix | P2 | Done ‚úÖ |

---

## v0.3.0 ‚Äî M3: Production Hardening ‚úÖ

**Status**: Complete

### Deliverables

| Feature | Acceptance Criteria | Status | Verify |
|---------|-------------------|--------|--------|
| Read-only replica enforcement | DDL/DML writes rejected on replica (SQLSTATE `25006`) | ‚úÖ | `cargo test -p falcon_server -- read_only` |
| Graceful shutdown | SIGTERM ‚Üí drain timeout ‚Üí force close | ‚úÖ | `cargo test -p falcon_protocol_pg -- shutdown` |
| Health checks | HTTP `/health`, `/ready`, `/status` endpoints | ‚úÖ | `cargo test -p falcon_server -- health` |
| Query timeout | `SET statement_timeout`; SQLSTATE `57014` on expiry | ‚úÖ | `cargo test -p falcon_protocol_pg -- statement_timeout` |
| Connection limits | `max_connections` enforced; SQLSTATE `53300` | ‚úÖ | `cargo test -p falcon_protocol_pg -- max_connections` |
| Idle timeout | Idle connections closed after configurable period | ‚úÖ | `cargo test -p falcon_protocol_pg -- idle_timeout` |
| TLS/SSL | SSLRequest ‚Üí TLS handshake; cert/key config | ‚úÖ | `cargo test -p falcon_protocol_pg -- tls` |
| Cancel request | PG cancel protocol (backend key ‚Üí kill query) | ‚ö†Ô∏è Partial | Accepted but not acted upon |
| Plan cache | LRU cache with `SHOW falcon.plan_cache` | ‚úÖ | `cargo test -p falcon_protocol_pg -- plan_cache` |
| Slow query log | `SET log_min_duration_statement`, `SHOW falcon.slow_queries` | ‚úÖ | `cargo test -p falcon_protocol_pg -- slow_query` |

### Acceptance Gates

1. All M2 gates still pass.
2. Health endpoint returns 503 during graceful shutdown drain.
3. TLS connection via `psql "sslmode=require"` succeeds (when cert/key configured).

---

## v0.4.0 ‚Äî M4: Analytics & Multi-Tenancy ‚úÖ

**Status**: Complete

### Deliverables

| Feature | Acceptance Criteria | Status | Verify |
|---------|-------------------|--------|--------|
| Columnstore storage | `ColumnStoreTable` with frozen segments, write buffer, `column_scan` | ‚úÖ | `cargo test -p falcon_storage -- columnstore` |
| Analytics node role | `NodeRole::Analytics` enforced; ColumnStore allowed on Analytics/Standalone only | ‚úÖ | `cargo test -p falcon_common -- node_role` |
| `scan_columnar` API | `StorageEngine::scan_columnar()` returns `Vec<Vec<Datum>>` per column | ‚úÖ | `cargo test -p falcon_storage` |
| Vectorized execution | `RecordBatch::from_columns`, `vectorized_filter`, `vectorized_aggregate`, `vectorized_project`, `vectorized_hash_join`, `vectorized_sort` | ‚úÖ | `cargo test -p falcon_executor -- vectorized` |
| Columnar aggregate path | `exec_columnar_aggregate`: COUNT/SUM/AVG/MIN/MAX over ColumnStore bypass row-at-a-time | ‚úÖ | `cargo test -p falcon_protocol_pg -- columnstore` |
| Multi-tenancy: CREATE TENANT | `CREATE TENANT name [MAX_QPS n] [MAX_STORAGE_BYTES n]` SQL command | ‚úÖ | `cargo test -p falcon_protocol_pg -- create_tenant` |
| Multi-tenancy: DROP TENANT | `DROP TENANT name` SQL command with existence check | ‚úÖ | `cargo test -p falcon_protocol_pg -- drop_tenant` |
| Multi-tenancy: SHOW tenants | `SHOW falcon.tenants` ‚Äî lists all registered tenants | ‚úÖ | `cargo test -p falcon_protocol_pg -- show_falcon_tenants` |
| Multi-tenancy: SHOW tenant_usage | `SHOW falcon.tenant_usage` ‚Äî per-tenant metrics (txns, QPS, memory) | ‚úÖ | `cargo test -p falcon_protocol_pg -- show_falcon_tenant_usage` |
| Tenant quota enforcement | `TenantRegistry::check_begin_txn` enforces QPS, txn limits, status | ‚úÖ | `cargo test -p falcon_storage -- tenant` |
| Duplicate tenant guard | `register_tenant` rejects duplicate names (case-insensitive) | ‚úÖ | `cargo test -p falcon_protocol_pg -- duplicate` |
| Cross-region lag routing | `DistributedQueryEngine::select_least_lagging_replica` + `route_read_to_replica` | ‚úÖ | `cargo test -p falcon_protocol_pg -- lagging_replica` |
| BoundStatement variants | `ShowTenants`, `ShowTenantUsage`, `CreateTenant`, `DropTenant` in binder/planner/executor | ‚úÖ | `cargo build --workspace` |

### Acceptance Gates

1. **Columnar COUNT(*)**: `SELECT COUNT(*) FROM t` on a ColumnStore table returns correct result via vectorized path (no row-at-a-time scan).
2. **Columnar SUM**: `SELECT SUM(col) FROM t` on a ColumnStore table returns correct aggregate.
3. **CREATE TENANT round-trip**: `CREATE TENANT acme` ‚Üí `SHOW falcon.tenants` shows `acme` ‚Üí `DROP TENANT acme` removes it.
4. **Duplicate tenant rejected**: Second `CREATE TENANT acme` returns `ERROR 42710`.
5. **Lag-aware routing**: `select_least_lagging_replica` prefers `lag_lsn=0` over lagging replicas; skips disconnected replicas.
6. **No regressions**: `cargo test --workspace` passes (excluding pre-existing GC benchmark timing flake).

### Verification Commands

```bash
cargo test -p falcon_protocol_pg -- test_columnstore_count_uses_vectorized_path
cargo test -p falcon_protocol_pg -- test_columnstore_sum_uses_vectorized_path
cargo test -p falcon_protocol_pg -- test_create_tenant_basic
cargo test -p falcon_protocol_pg -- test_create_tenant_duplicate_fails
cargo test -p falcon_protocol_pg -- test_drop_tenant_basic
cargo test -p falcon_protocol_pg -- test_show_falcon_tenants
cargo test -p falcon_protocol_pg -- test_show_falcon_tenant_usage
cargo test -p falcon_protocol_pg -- test_select_least_lagging_replica_prefers_caught_up
cargo test -p falcon_protocol_pg -- test_select_least_lagging_replica_skips_disconnected
cargo test -p falcon_protocol_pg -- test_select_least_lagging_replica_empty_returns_none
cargo test --workspace
```

### Implementation Notes

- **Vectorized columnar path** (`executor_columnar.rs`): triggered when table is ColumnStore, query is a pure aggregate (no GROUP BY, no HAVING, no correlated subqueries). Falls back to `exec_aggregate` for complex cases.
- **`scan_columnar`** (`engine_dml.rs`): reads from both frozen segments and write buffer via `ColumnStoreTable::column_scan`.
- **Tenant SQL commands** are intercepted in `handle_system_query` before the SQL parser (non-standard syntax). `SHOW falcon.tenants` / `SHOW falcon.tenant_usage` go through the standard `handle_show_falcon` dispatch.
- **Lag-aware routing** (`query_engine.rs`): `route_read_to_replica` selects the shard with the lowest `lag_lsn` within a configurable threshold, falling back to shard 0 (primary) when no replica qualifies.

---

## Release Cadence

| Version | Status | Key Metric |
|---------|--------|------------|
| v0.1.0 | ‚úÖ Done | 1,081 tests, MVCC + WAL + failover |
| v0.2.0 | ‚úÖ Done | gRPC WAL streaming, two-node e2e |
| v0.3.0 | ‚úÖ Done | Health checks, TLS, query timeout, plan cache |
| v0.4.0 | ‚úÖ Done | Columnstore vectorized agg + multi-tenancy + lag-aware routing |
| **v0.4.x** | ‚úÖ Done | Production Gate Alpha ‚Äî unwrap=0, error model, crash domain |
| **v0.5.0** | ‚úÖ Done | Operationally Usable ‚Äî cluster admin, rebalance, scale-out/in |
| **v0.6.0** | ‚úÖ Done | Latency-Controlled OLTP ‚Äî backpressure, admission, memory budget |
| **v0.7.0** | ‚úÖ Done | Deterministic Transactions ‚Äî 2PC hardening, in-doubt resolver |
| **v0.8.0** | ‚úÖ Done | Chaos-Ready ‚Äî chaos matrix, fault injection, chaos CI |
| **v0.9.0** | ‚úÖ Done | Production Candidate ‚Äî semantic freeze, wire versioning, rolling upgrade |
| **v1.0 Phase 1** | ‚úÖ Done | LSM kernel ‚Äî 1,917 tests, disk-backed OLTP, MVCC encoding, idempotency |
| **v1.0 Phase 2** | ‚úÖ Done | SQL completeness ‚Äî 1,976 tests, DECIMAL, composite indexes, RBAC, txn control |
| **1.0.0-rc.1** | ‚úÖ Done | Version aligned, code audit fixes, e2e evidence, RBAC enforcement matrix |
| **v2.0 Phase 3** | ‚úÖ Done | Enterprise ‚Äî 2,056 tests, RLS, TDE, partitioning, PITR, CDC |
| **Storage Hardening** | ‚úÖ Done | 7 modules, 61 tests ‚Äî WAL recovery, compaction, memory budget, GC safepoint, fault injection |
| **Distributed Hardening** | ‚úÖ Done | 6 modules, 62 tests ‚Äî epoch fencing, leader lease, migration, throttle, supervisor |
| **Native Protocol** | ‚úÖ Done | 2,239 tests ‚Äî binary protocol, JDBC driver, compression, HA failover |
| **v1.0.0** | üìã Planned | Production-Grade Database Kernel ‚Äî all gates pass |

---

## Version Philosophy

> **0.x = Engineering and semantics not yet frozen, but already usable in real production.**
> **1.0.0 = Semantic freeze + operational closure + tail latency controlled.**

From v0.4.0 onward:
- ‚ùå Breaking existing semantics without a spec/invariant/test is forbidden.
- ‚úÖ Every behavioral change must have a spec, invariant, and test.
- Every release must have a **hard Gate** and a **reproducible exercise**.
- All consistency and reliability guarantees must be **provable, replayable, and verifiable**.

> **FalconDB's 1.0 is not "features complete" ‚Äî it is "failure behavior completely defined."**

---

## v0.4.x ‚Äî Production Hardening Alpha

**Status**: ‚úÖ Complete
**Positioning**: For engineers and early pilot users. Can run in test / non-critical production. Tail latency not yet guaranteed.

### Core Objective
> Upgrade "it runs" to "it won't crash randomly, errors are understandable, failures are recoverable."

### Problems to Solve
- Core path `unwrap` / `expect` = **0**
- Unified error model (`UserError` / `Retryable` / `Transient` / `InternalBug`)
- Crash domain control (single request/connection/transaction failure ‚â† process crash)
- Initial Production Gate

### Deliverables

| Item | Location | Status |
|------|----------|--------|
| `FalconError` unified model + `ErrorKind` + `FalconResult` | `falcon_common::error` | ‚úÖ |
| `bail_user!` / `bail_retryable!` / `bail_transient!` macros | `falcon_common::error` | ‚úÖ |
| `ErrorContext` trait (`.ctx()` / `.ctx_with()`) | `falcon_common::error` | ‚úÖ |
| SQLSTATE + error code mapping | `falcon_common::error` | ‚úÖ |
| `deny_unwrap.sh` ‚Äî core crate scan | `scripts/deny_unwrap.sh` | ‚úÖ |
| `ci_production_gate.sh` ‚Äî minimum gate | `scripts/ci_production_gate.sh` | ‚úÖ |
| `ci_production_gate_v2.sh` ‚Äî 14-gate full gate | `scripts/ci_production_gate_v2.sh` | ‚úÖ |
| `install_panic_hook()` + `catch_request()` + `PanicThrottle` | `falcon_common::crash_domain` | ‚úÖ |
| `DiagBundle` + `DiagBundleBuilder` + JSON export | `falcon_common::diag_bundle` | ‚úÖ |
| Core path unwrap elimination (`handler.rs`, `query_engine.rs`, `online_ddl.rs`) | multiple | ‚úÖ |
| `docs/error_model.md` | `docs/` | ‚úÖ |

### Hard Gates

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | `deny_unwrap.sh` 0 hits in core crates | `bash scripts/deny_unwrap.sh` |
| G2 | `ci_failover_gate.sh` P0 all pass | `bash scripts/ci_failover_gate.sh` |
| G3 | Random `kill -9` leader ‚Üí no panic, system recovers | `ci_failover_gate.sh` |
| G4 | Malformed PG protocol input ‚Üí no panic, error returned | `cargo test -p falcon_common -- crash_domain` |
| G5 | `cargo test --workspace` 0 failures | `cargo test --workspace` |

---

## v0.5.0 ‚Äî Operationally Usable

**Status**: ‚úÖ Complete
**Positioning**: Usable in small-scale real production. Ops can intervene manually. Tail latency still not guaranteed.

### Core Objective
> "Not just the author can run it ‚Äî ops can run it, rescue it, and scale it."

### Problems to Solve
- Cluster ops closure: scale-out, scale-in, rebalance, leader transfer
- All state transitions must have: a state machine, be interruptible, be resumable
- Structured event log for all state transitions

### Deliverables

| Item | Location | Status |
|------|----------|--------|
| `local_cluster_harness.sh` ‚Äî 3-node start/stop/failover/smoke | `scripts/` | ‚úÖ |
| `rolling_upgrade_smoke.sh` | `scripts/` | ‚úÖ |
| Cluster admin CLI / API (`cluster status`, `add node`, `remove node`) | `falcon_server` | ‚úÖ |
| `rebalance plan` (dry-run) + `rebalance apply` | `falcon_cluster::rebalancer` | ‚úÖ |
| `promote leader` (per-shard) | `falcon_cluster::ha` | ‚úÖ |
| Scale-out state machine (join ‚Üí catch-up ‚Üí serve reads ‚Üí serve writes ‚Üí rebalance) | `falcon_cluster` | ‚úÖ |
| Scale-in state machine (drain ‚Üí move leadership ‚Üí move data ‚Üí remove membership) | `falcon_cluster` | ‚úÖ |
| Structured state transition event log | `falcon_cluster` | ‚úÖ |
| `ops_playbook.md` ‚Äî scale-out/in, failover, rolling upgrade | `docs/` | ‚úÖ |

### Hard Gates

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | `add node` ‚Üí rebalance ‚Üí data verified on new node | `local_cluster_harness.sh scaleout` |
| G2 | `remove node` ‚Üí no data loss | `local_cluster_harness.sh scalein` |
| G3 | Rolling restart (node by node) ‚Üí service not interrupted | `rolling_upgrade_smoke.sh` |
| G4 | RTO < 10s (write availability after leader death) | `ci_failover_gate.sh` |
| G5 | Rebalance plan output: shard moves, estimated data volume, estimated time | `SHOW falcon.rebalance_plan` (6 columns) |

---

## v0.6.0 ‚Äî Latency-Controlled OLTP

**Status**: ‚úÖ Complete
**Positioning**: True OLTP product. Can enter core business, but still requires monitoring.

### Core Objective
> "Not fast on average ‚Äî P99 doesn't explode."

### Problems to Solve
- Full-chain backpressure
- Admission control
- Global memory budget
- WAL / replication / executor queue visibility

### Deliverables

| Item | Location | Status |
|------|----------|--------|
| Connection / query / write / WAL / apply permits | `falcon_cluster::admission` | ‚úÖ |
| `MemoryBudget` (global + per-shard, soft 80% / hard 95%) | `falcon_cluster::admission` | ‚úÖ |
| `QueryGovernor` (time / rows / bytes / memory per query) | `falcon_executor::governor` | ‚úÖ |
| `ShardCircuitBreaker` + `ClusterCircuitBreaker` | `falcon_cluster::circuit_breaker` | ‚úÖ |
| Queue metrics: length / bytes / wait time / reject count | `falcon_cluster::admission` | ‚úÖ |
| Priority queues: OLTP (high) vs large query / DDL / rebalance (low) | `falcon_executor::priority_scheduler` | ‚úÖ |
| Token bucket for DDL / backfill / rebalance | `falcon_cluster::token_bucket` | ‚úÖ |
| Backpressure smoke in CI gate | `ci_production_gate_v2.sh` | ‚úÖ |

### Hard Gates

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | Small memory + sustained writes ‚Üí no OOM, stable rejection | `ci_production_gate_v2.sh --gate 9` |
| G2 | Mixed load (OLTP + large query): small txn P99 < 10ms | bench + gate |
| G3 | Write storm ‚Üí system enters stable backoff, not avalanche | admission tests |
| G4 | Memory budget metrics: `used / limit / over-limit count / wait time` all present | observability check |

---

## v0.7.0 ‚Äî Deterministic Transactions

**Status**: ‚úÖ Complete
**Positioning**: Enterprise-grade transaction kernel. Usable for cross-shard strong-consistency business.

### Core Objective
> "Failures are not random ‚Äî they are explainable and predictable."

### Problems to Solve
- 2PC tail latency
- In-doubt storm
- Transactions don't spiral under leader churn

### Deliverables

| Item | Location | Status |
|------|----------|--------|
| Commit decision durable point (coordinator spec) | `falcon_cluster::deterministic_2pc` | ‚úÖ |
| `TxnOutcomeCache` (TTL + max-size eviction) | `falcon_cluster::indoubt_resolver` | ‚úÖ |
| `InDoubtResolver` background task (rate-limited, observable) | `falcon_cluster::indoubt_resolver` | ‚úÖ |
| Layered timeout: soft ‚Üí `Retryable`, hard ‚Üí abort + resolver | `falcon_cluster::deterministic_2pc` | ‚úÖ |
| Slow-shard policy (configurable: fast-abort or hedged request) | `falcon_cluster::deterministic_2pc` | ‚úÖ |
| `RecoveryTracker` (5-phase structured startup logging) | `falcon_common::request_context` | ‚úÖ |
| `StageLatency` per-query breakdown | `falcon_common::request_context` | ‚úÖ |

### Hard Gates

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | 2PC: kill coordinator after prepare ‚Üí in-doubt resolver converges within 30s | `ci_failover_gate.sh` |
| G2 | Leader churn: P99 does not grow unboundedly | bench under churn |
| G3 | All in-doubt transactions converge (0 stuck after 60s) | `indoubt_resolver` tests |
| G4 | `TxnOutcomeCache` hit rate > 90% under coordinator-query storm | cache tests |

---

## v0.8.0 ‚Äî Chaos-Ready

**Status**: ‚úÖ Complete
**Positioning**: Acceptable to cloud vendors / large enterprises.

### Core Objective
> "Every failure you're afraid of ‚Äî we've drilled it."

### Problems to Solve
- Chaos injection coverage
- Failure coverage matrix
- Real-world failure modeling

### Deliverables

| Item | Location | Status |
|------|----------|--------|
| `chaos_injector.sh` (kill / restart / delay / loss / report) | `scripts/` | ‚úÖ |
| `chaos_matrix.md` (30 scenarios, 6 categories, acceptance thresholds) | `docs/` | ‚úÖ |
| Chaos + workload joint exercise (30‚Äì60s CI smoke) | `ci_production_gate_v2.sh` | ‚úÖ |
| In-process fault injection (`falcon_cluster::fault_injection`) | `falcon_cluster` | ‚úÖ |
| Chaos report auto-generation | `chaos_injector.sh report` | ‚úÖ |
| Network partition simulation (split-brain) | `falcon_cluster::fault_injection` | ‚úÖ |
| CPU / IO jitter injection | `falcon_cluster::fault_injection` | ‚úÖ |

### Hard Gates

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | Chaos running: 0 panics | `ci_production_gate_v2.sh --gate 6` |
| G2 | Chaos running: data consistent after convergence | consistency check |
| G3 | Chaos running: in-doubt txns all converge | `indoubt_resolver` metrics |
| G4 | Chaos report auto-generated as CI artifact | `chaos_injector.sh report` |
| G5 | All 30 chaos matrix scenarios documented with expected behavior | `chaos_matrix.md` |

---

## v0.9.0 ‚Äî Production Candidate

**Status**: ‚úÖ Complete
**Positioning**: Pre-1.0 freeze. Bug fixes only, no new semantics.

### Core Objective
> "From today, semantics no longer change arbitrarily."

### Problems to Solve
- Semantic freeze
- On-disk / wire versioning
- Rolling upgrade

### Deliverables

| Item | Location | Status |
|------|----------|--------|
| Wire compatibility policy (document + enforce) | `docs/wire_compatibility.md` | ‚úÖ |
| WAL / snapshot versioning (magic + version header) | `falcon_storage::wal` | ‚úÖ |
| `rolling_upgrade_smoke.sh` (3-node rolling upgrade) | `scripts/` | ‚úÖ |
| `ops_playbook.md` (complete) | `docs/` | ‚úÖ |
| `CHANGELOG.md` with semantic versioning | root | ‚úÖ |
| Backward-compatible config schema (deprecated fields warned, not errored) | `falcon_common::config` | ‚úÖ |

### Hard Gates

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | v0.9.x ‚Üí v0.9.y rolling upgrade succeeds (no downtime) | `rolling_upgrade_smoke.sh` |
| G2 | Rollback feasible (downgrade v0.9.y ‚Üí v0.9.x) | manual exercise |
| G3 | Wire protocol: v0.9.x client connects to v0.9.y server | compatibility test |
| G4 | WAL written by v0.9.x readable by v0.9.y | WAL replay test |
| G5 | All v0.8.0 gates still pass | full gate suite |

---

## v1.0.0 ‚Äî Production-Grade Database Kernel

**Status**: üìã Planned
**Positioning**: "I dare to tell users: this is a production database."

### Core Objective
> All gates pass. Production readiness ‚â• 9.8/10. Docs, exercises, and diagnostics complete.

### Must Have

| Requirement | Criterion |
|-------------|-----------|
| All gates pass | Every gate from v0.4.x through v0.9.0 passes |
| `deny_unwrap.sh` | 0 hits in all core crates |
| `cargo test --workspace` | 0 failures |
| `ci_production_gate_v2.sh` | All 14 gates pass |
| `ci_failover_gate.sh` | All P0 gates pass |
| Tail latency | P99 < 10ms (OLTP point lookup, no chaos) |
| Failover RTO | < 5s (quorum-ack mode) |
| RPO | 0 bytes (quorum-ack mode) |
| In-doubt convergence | < 30s after coordinator restart |
| Panic count | 0 during any gate exercise |

### Final Deliverables

| Document | Status |
|----------|--------|
| `docs/production_readiness_report.md` | ‚úÖ (updated each release) |
| `docs/chaos_matrix.md` + chaos report | ‚úÖ |
| `docs/ops_playbook.md` | ‚úÖ |
| `docs/error_model.md` | ‚úÖ |
| `docs/observability.md` | ‚úÖ |
| `performance_baseline.md` | ‚úÖ |
| `CHANGELOG.md` | ‚úÖ |

### 1.0 Definition

| Property | Definition |
|----------|------------|
| **Behavior stable** | No semantic changes without a versioned spec |
| **Failures predictable** | Every error has an `ErrorKind`, SQLSTATE, and retry hint |
| **Ops operable** | Any trained ops engineer can scale, failover, and diagnose |
| **Tail latency controlled** | P99 bounded under defined load profiles |
| **Failure behavior defined** | Every failure mode in `chaos_matrix.md` has documented expected behavior |

---

## v1.0 ‚Äî Phase 1: Industrial/Financial-Grade OLTP Kernel

**Status**: ‚úÖ Complete  
**Goal**: Disk-backed OLTP, deterministic txn semantics, predictable P99, faster than PG

### P0: Storage & Transaction Kernel

| Deliverable | Location | Status |
|------------|----------|--------|
| LSM storage engine (WAL‚ÜíMemTable‚ÜíFlush‚ÜíL0‚ÜíCompaction) | `falcon_storage::lsm::engine` | ‚úÖ |
| SST file format (data blocks + index + bloom filter + footer) | `falcon_storage::lsm::sst` | ‚úÖ |
| Bloom filter (per-SST negative lookup elimination) | `falcon_storage::lsm::bloom` | ‚úÖ |
| LRU block cache (configurable budget + eviction metrics) | `falcon_storage::lsm::block_cache` | ‚úÖ |
| Leveled compaction (L0‚ÜíL1 merge, dedup, tombstone removal) | `falcon_storage::lsm::compaction` | ‚úÖ |
| LSM memtable (sorted BTreeMap, freeze/flush lifecycle) | `falcon_storage::lsm::memtable` | ‚úÖ |
| MVCC value encoding (txn_id, status, commit_ts, data) | `falcon_storage::lsm::mvcc_encoding` | ‚úÖ |
| Visibility rules (Prepared=invisible, Committed=ts-gated) | `MvccValue::is_visible()` | ‚úÖ |
| Persistent txn_meta store (txn_id‚Üíoutcome for 2PC recovery) | `falcon_storage::lsm::mvcc_encoding::TxnMetaStore` | ‚úÖ |
| LSM backpressure (memtable budget flush, L0 stall trigger) | `LsmEngine::maybe_stall_writes()` | ‚úÖ |

### P1: Performance, Idempotency, Audit, Benchmark

| Deliverable | Location | Status |
|------------|----------|--------|
| Idempotency key store (persistent key‚Üíresult, TTL/GC) | `falcon_storage::lsm::idempotency` | ‚úÖ |
| Transaction-level audit log (txn_id, keys, outcome, epoch) | `falcon_storage::lsm::txn_audit` | ‚úÖ |
| TPC-B (pgbench) benchmark workload | `falcon_bench --tpcb` | ‚úÖ |
| LSM disk-backed KV benchmark | `falcon_bench --lsm` | ‚úÖ |
| P50/P95/P99/Max latency + backpressure reporting | `falcon_bench` all workloads | ‚úÖ |
| CI Phase 1 gates script | `scripts/ci_phase1_gates.sh` | ‚úÖ |

### Test Coverage

| Module | Tests |
|--------|-------|
| `lsm::bloom` | 4 |
| `lsm::block_cache` | 5 |
| `lsm::sst` | 5 |
| `lsm::memtable` | 7 |
| `lsm::compaction` | 4 |
| `lsm::engine` | 8 |
| `lsm::mvcc_encoding` | 15 |
| `lsm::idempotency` | 10 |
| `lsm::txn_audit` | 10 |
| **Total new** | **68** |

### Hard Gates (v1.0 Release)

| Gate | Criterion | Verify |
|------|-----------|--------|
| G1 | Data > 2√ó memory budget, 10min write, no OOM | `ci_phase1_gates.sh gate2` |
| G2 | kill -9 ‚Üí restart ‚Üí data intact | `ci_phase1_gates.sh gate3` |
| G3 | Prepared versions never visible to readers | `test_mvcc_visibility_prepared_never_visible` |
| G4 | In-doubt txn converges after crash | `TxnMetaStore` persistence tests |
| G5 | TPC-B P95/P99 reported, backpressure counted | `falcon_bench --tpcb --export json` |
| G6 | Error model 4-tier full coverage | `ci_phase1_gates.sh gate1` |

---

## v1.0 ‚Äî Phase 2: SQL Completeness & Enterprise Kernel

**Status**: ‚úÖ Complete  
**Goal**: Full SQL type coverage, constraint enforcement, transaction control, fine-grained access control, and operational robustness.

### P0 v1.1: DECIMAL Type

| Deliverable | Location | Status |
|------------|----------|--------|
| `Datum::Decimal(i128, u8)` ‚Äî mantissa + scale | `falcon_common::datum` | ‚úÖ |
| `DataType::Decimal(u8, u8)` ‚Äî precision + scale | `falcon_common::types` | ‚úÖ |
| Decimal encoding in secondary index keys | `falcon_storage::memtable` | ‚úÖ |
| Decimal in stats, sharding, gather, JSONB, COPY, PG wire | multiple | ‚úÖ |
| 11 tests | `falcon_common::datum` | ‚úÖ |

### P0 v1.2: Composite / Covering / Prefix Indexes

| Deliverable | Location | Status |
|------------|----------|--------|
| `SecondaryIndex::column_indices: Vec<usize>` (multi-column) | `falcon_storage::memtable` | ‚úÖ |
| `encode_key()` ‚Äî composite key encoding by column concatenation | `falcon_storage::memtable` | ‚úÖ |
| `new_composite()`, `new_covering()`, `new_prefix()` constructors | `falcon_storage::memtable` | ‚úÖ |
| `prefix_scan()` for prefix index range queries | `falcon_storage::memtable` | ‚úÖ |
| `create_composite_index`, `create_covering_index`, `create_prefix_index` | `falcon_storage::engine_ddl` | ‚úÖ |
| Backfill from existing table data on index creation | `falcon_storage::engine_ddl` | ‚úÖ |
| 10 tests (encoding, uniqueness, prefix truncation, insert/remove) | `falcon_storage::memtable` | ‚úÖ |

### P0 v1.2: CHECK Constraint Runtime Enforcement

| Deliverable | Location | Status |
|------------|----------|--------|
| `ExecutionError::CheckConstraintViolation(String)` error variant | `falcon_common::error` | ‚úÖ |
| SQLSTATE `23514` (`CHECK_VIOLATION`) constant | `falcon_common::consistency` | ‚úÖ |
| CHECK constraint evaluation on INSERT / UPDATE | `falcon_executor::executor_subquery` | ‚úÖ |
| CHECK + NOT NULL enforcement in `exec_insert_select` | `falcon_executor::executor_dml` | ‚úÖ |

### P0 v1.2: Transaction READ ONLY / READ WRITE + Timeout + Exec Summary

| Deliverable | Location | Status |
|------------|----------|--------|
| `TxnHandle::read_only: bool` ‚Äî per-transaction access mode | `falcon_txn::manager` | ‚úÖ |
| `TxnHandle::timeout_ms: u64` ‚Äî per-transaction timeout | `falcon_txn::manager` | ‚úÖ |
| `TxnHandle::exec_summary: TxnExecSummary` ‚Äî statement/row counters | `falcon_txn::manager` | ‚úÖ |
| `TxnHandle::is_timed_out()`, `elapsed_ms()` helper methods | `falcon_txn::manager` | ‚úÖ |
| `TxnExecSummary` ‚Äî `record_read/insert/update/delete/statement()` | `falcon_txn::manager` | ‚úÖ |
| `reject_if_txn_read_only()` guard in INSERT / UPDATE / DELETE | `falcon_executor::executor` | ‚úÖ |
| `check_txn_timeout()` guard in INSERT / UPDATE / DELETE | `falcon_executor::executor` | ‚úÖ |
| 8 tests | `falcon_txn::tests` | ‚úÖ |

### P1 v1.3: Query Governor v2 ‚Äî Structured Abort Reasons

| Deliverable | Location | Status |
|------------|----------|--------|
| `GovernorAbortReason` enum (RowLimit / ByteLimit / Timeout / Memory) | `falcon_executor::governor` | ‚úÖ |
| `QueryGovernor::check_all_v2()` ‚Äî returns structured abort reason | `falcon_executor::governor` | ‚úÖ |
| `QueryGovernor::abort_reason()` ‚Äî current abort reason or None | `falcon_executor::governor` | ‚úÖ |
| 6 tests | `falcon_executor::governor` | ‚úÖ |

### P1 v1.4: Fine-Grained Admission Control

| Deliverable | Location | Status |
|------------|----------|--------|
| `OperationType` enum (Read / Write / Ddl) | `falcon_cluster::admission` | ‚úÖ |
| `DdlPermit` RAII guard ‚Äî auto-releases DDL slot on drop | `falcon_cluster::admission` | ‚úÖ |
| `AdmissionControl::acquire_ddl()` ‚Äî DDL concurrency limit | `falcon_cluster::admission` | ‚úÖ |
| `AdmissionConfig::max_inflight_ddl` (default: 4) | `falcon_cluster::admission` | ‚úÖ |
| `AdmissionMetrics::ddl_rejected` counter | `falcon_cluster::admission` | ‚úÖ |
| 2 tests | `falcon_cluster::admission` | ‚úÖ |

### P1 v1.5: Node Operational Mode (Drain / ReadOnly / Normal)

| Deliverable | Location | Status |
|------------|----------|--------|
| `NodeOperationalMode` enum (Normal / ReadOnly / Drain) | `falcon_cluster::cluster_ops` | ‚úÖ |
| `NodeModeController` ‚Äî thread-safe mode transitions with event logging | `falcon_cluster::cluster_ops` | ‚úÖ |
| `allows_reads()`, `allows_writes()`, `allows_ddl()` enforcement methods | `falcon_cluster::cluster_ops` | ‚úÖ |
| `drain()`, `set_read_only()`, `resume()` convenience methods | `falcon_cluster::cluster_ops` | ‚úÖ |
| All mode transitions logged to `ClusterEventLog` | `falcon_cluster::cluster_ops` | ‚úÖ |
| No-op detection (same-mode transitions not logged) | `falcon_cluster::cluster_ops` | ‚úÖ |
| 6 tests | `falcon_cluster::cluster_ops` | ‚úÖ |

### P1 v1.6: Role-Based Access Control + Schema Permissions

| Deliverable | Location | Status |
|------------|----------|--------|
| `RoleCatalog` ‚Äî in-memory role store with CRUD | `falcon_common::security` | ‚úÖ |
| Transitive role inheritance (`effective_roles()`) | `falcon_common::security` | ‚úÖ |
| Circular inheritance detection (rejected with error) | `falcon_common::security` | ‚úÖ |
| `grant_role()` / `revoke_role()` membership management | `falcon_common::security` | ‚úÖ |
| `PrivilegeManager` ‚Äî GRANT / REVOKE on tables, schemas, functions | `falcon_common::security` | ‚úÖ |
| `check_privilege()` ‚Äî checks effective role set against grants | `falcon_common::security` | ‚úÖ |
| `DefaultPrivilege` + `add_schema_default()` ‚Äî ALTER DEFAULT PRIVILEGES | `falcon_common::security` | ‚úÖ |
| Duplicate grant deduplication | `falcon_common::security` | ‚úÖ |
| `revoke_all_on_object()` ‚Äî used on DROP TABLE/SCHEMA | `falcon_common::security` | ‚úÖ |
| 17 tests (9 RoleCatalog + 8 PrivilegeManager) | `falcon_common::security` | ‚úÖ |

### Phase 2 Test Coverage Summary

| Feature Area | New Tests |
|-------------|-----------|
| `Datum::Decimal` | 11 |
| Composite / covering / prefix indexes | 10 |
| Transaction READ ONLY + timeout + exec summary | 8 |
| Query Governor v2 (abort reasons) | 6 |
| Node operational mode | 6 |
| Fine-grained admission (DDL permits) | 2 |
| RoleCatalog (transitive inheritance) | 9 |
| PrivilegeManager (GRANT/REVOKE) | 8 |
| **Total new (Phase 2)** | **60** |

### Verification

```bash
cargo test --workspace   # 2,239 pass, 0 failures
cargo test -p falcon_common --lib -- security::tests
cargo test -p falcon_txn --lib -- txn_manager_tests::test_txn_
cargo test -p falcon_executor --lib -- governor::tests
cargo test -p falcon_cluster --lib -- cluster_ops::tests
cargo test -p falcon_cluster --lib -- admission::tests
cargo test -p falcon_storage --lib -- memtable::index_tests
```

---

## v2.0 ‚Äî Phase 3: Enterprise Edition Features ‚úÖ

**Status**: Implemented

**Positioning**: Enterprise-grade capabilities for production deployments ‚Äî multi-tenant
data isolation, compliance, disaster recovery, and real-time integration.

### P0 v2.1: Row-Level Security (RLS)

| Deliverable | Location | Status |
|------------|----------|--------|
| `RlsPolicy` ‚Äî per-table policy with USING/WITH CHECK expressions | `falcon_common::rls` | ‚úÖ |
| `PolicyCommand` ‚Äî ALL/SELECT/INSERT/UPDATE/DELETE targeting | `falcon_common::rls` | ‚úÖ |
| `PolicyPermissiveness` ‚Äî PERMISSIVE (OR) / RESTRICTIVE (AND) | `falcon_common::rls` | ‚úÖ |
| `RlsPolicyManager` ‚Äî create/drop policies, enable/disable/force RLS | `falcon_common::rls` | ‚úÖ |
| `combined_using_expr()` ‚Äî PG-compatible OR+AND policy combination | `falcon_common::rls` | ‚úÖ |
| `combined_check_expr()` ‚Äî write-side policy combination | `falcon_common::rls` | ‚úÖ |
| `should_bypass()` ‚Äî superuser/owner bypass logic (respects FORCE) | `falcon_common::rls` | ‚úÖ |
| Role-scoped policies (PUBLIC or specific roles) | `falcon_common::rls` | ‚úÖ |
| `drop_all_policies()` ‚Äî cleanup on DROP TABLE | `falcon_common::rls` | ‚úÖ |
| Wired into `StorageEngine.rls_manager` | `falcon_storage::engine` | ‚úÖ |
| 15 tests | `falcon_common::rls` | ‚úÖ |

### P0 v2.2: Transparent Data Encryption (TDE)

| Deliverable | Location | Status |
|------------|----------|--------|
| `EncryptionKey` ‚Äî AES-256 key with PBKDF2 derivation | `falcon_storage::encryption` | ‚úÖ |
| `KeyManager` ‚Äî master key + DEK lifecycle management | `falcon_storage::encryption` | ‚úÖ |
| `DekId` / `WrappedDek` ‚Äî encrypted DEK storage with nonce | `falcon_storage::encryption` | ‚úÖ |
| `EncryptionScope` ‚Äî per-WAL/table/SST/backup key isolation | `falcon_storage::encryption` | ‚úÖ |
| `encrypt_block()` / `decrypt_block()` ‚Äî data encryption primitives | `falcon_storage::encryption` | ‚úÖ |
| `rotate_master_key()` ‚Äî re-wrap all DEKs with new passphrase | `falcon_storage::encryption` | ‚úÖ |
| Wired into `StorageEngine.key_manager` | `falcon_storage::engine` | ‚úÖ |
| 11 tests | `falcon_storage::encryption` | ‚úÖ |

### P0 v2.3: Table Partitioning (Range / Hash / List)

| Deliverable | Location | Status |
|------------|----------|--------|
| `PartitionStrategy` ‚Äî Range / Hash / List strategies | `falcon_storage::partition` | ‚úÖ |
| `RangeBound` ‚Äî inclusive lower / exclusive upper with MINVALUE/MAXVALUE | `falcon_storage::partition` | ‚úÖ |
| `ListBound` ‚Äî explicit value set matching | `falcon_storage::partition` | ‚úÖ |
| `PartitionManager` ‚Äî create/drop/attach/detach partitions | `falcon_storage::partition` | ‚úÖ |
| `route()` ‚Äî datum-based partition routing for INSERT | `falcon_storage::partition` | ‚úÖ |
| `prune_range()` ‚Äî partition pruning for range scans | `falcon_storage::partition` | ‚úÖ |
| `prune_list()` ‚Äî partition pruning for IN-list scans | `falcon_storage::partition` | ‚úÖ |
| Default partition (catches unrouted rows) | `falcon_storage::partition` | ‚úÖ |
| Wired into `StorageEngine.partition_manager` | `falcon_storage::engine` | ‚úÖ |
| 10 tests | `falcon_storage::partition` | ‚úÖ |

### P1 v2.4: Point-in-Time Recovery (PITR)

| Deliverable | Location | Status |
|------------|----------|--------|
| `Lsn` ‚Äî WAL position type with PG-compatible display format | `falcon_storage::pitr` | ‚úÖ |
| `RecoveryTarget` ‚Äî Latest / Time / LSN / XID / RestorePoint | `falcon_storage::pitr` | ‚úÖ |
| `WalArchiver` ‚Äî WAL segment archiving with retention policies | `falcon_storage::pitr` | ‚úÖ |
| `BaseBackup` ‚Äî consistent snapshot metadata with LSN tracking | `falcon_storage::pitr` | ‚úÖ |
| `RestorePoint` ‚Äî named recovery points (pg_create_restore_point) | `falcon_storage::pitr` | ‚úÖ |
| `RecoveryExecutor` ‚Äî coordinated replay with target detection | `falcon_storage::pitr` | ‚úÖ |
| `find_base_backup()` ‚Äî optimal backup selection for recovery target | `falcon_storage::pitr` | ‚úÖ |
| `segments_for_recovery()` ‚Äî WAL segment range computation | `falcon_storage::pitr` | ‚úÖ |
| `apply_retention()` ‚Äî time-based segment cleanup | `falcon_storage::pitr` | ‚úÖ |
| Wired into `StorageEngine.wal_archiver` | `falcon_storage::engine` | ‚úÖ |
| 10 tests | `falcon_storage::pitr` | ‚úÖ |

### P1 v2.5: Change Data Capture (CDC) / Logical Decoding

| Deliverable | Location | Status |
|------------|----------|--------|
| `ReplicationSlot` ‚Äî consumer position tracking with activate/deactivate | `falcon_storage::cdc` | ‚úÖ |
| `ChangeEvent` ‚Äî structured INSERT/UPDATE/DELETE/DDL/COMMIT events | `falcon_storage::cdc` | ‚úÖ |
| `CdcManager` ‚Äî slot management + bounded event ring buffer | `falcon_storage::cdc` | ‚úÖ |
| `emit_insert/update/delete/commit()` ‚Äî convenience emitters | `falcon_storage::cdc` | ‚úÖ |
| `poll_changes()` ‚Äî consumer polling with slot-scoped progress | `falcon_storage::cdc` | ‚úÖ |
| `advance_slot()` ‚Äî consumer acknowledges processed LSN | `falcon_storage::cdc` | ‚úÖ |
| Table filtering per slot | `falcon_storage::cdc` | ‚úÖ |
| Old row values (REPLICA IDENTITY FULL support) | `falcon_storage::cdc` | ‚úÖ |
| Buffer eviction for bounded memory | `falcon_storage::cdc` | ‚úÖ |
| Wired into `StorageEngine.cdc_manager` | `falcon_storage::engine` | ‚úÖ |
| 9 tests | `falcon_storage::cdc` | ‚úÖ |

### Phase 3 Test Coverage Summary

| Feature Area | New Tests |
|-------------|-----------|
| Row-Level Security (RLS) | 15 |
| Transparent Data Encryption (TDE) | 11 |
| Table Partitioning (Range/Hash/List) | 10 |
| Point-in-Time Recovery (PITR) | 10 |
| Change Data Capture (CDC) | 9 |
| **Total new (Phase 3)** | **55** |

### Verification

```bash
cargo test --workspace   # 2,239 pass, 0 failures
cargo test -p falcon_common --lib -- rls::tests
cargo test -p falcon_storage --lib -- encryption::tests
cargo test -p falcon_storage --lib -- partition::tests
cargo test -p falcon_storage --lib -- pitr::tests
cargo test -p falcon_storage --lib -- cdc::tests
```

---

## Storage Hardening ‚úÖ

**Status**: Complete  
**Goal**: Production-grade storage reliability ‚Äî WAL recovery, compaction scheduling, memory budget, GC safepoint, fault injection, offline diagnostics.

### Deliverables (7 modules, 61 tests)

| Module | Location | Tests | Status |
|--------|----------|------:|--------|
| Graded WAL error types + `CorruptionLog` | `falcon_storage::storage_error` | 6 | ‚úÖ |
| Phased WAL recovery (Scan‚ÜíApply‚ÜíValidate) | `falcon_storage::recovery` | 10 | ‚úÖ |
| Resource-isolated compaction scheduling | `falcon_storage::compaction_scheduler` | 11 | ‚úÖ |
| Unified memory budget (5 categories, 3 levels) | `falcon_storage::memory_budget` | 10 | ‚úÖ |
| GC safepoint unification + long-txn diagnostics | `falcon_storage::gc_safepoint` | 10 | ‚úÖ |
| Offline diagnostic tools (sst_verify, wal_inspect) | `falcon_storage::storage_tools` | 6 | ‚úÖ |
| Storage fault injection (6 fault types) | `falcon_storage::storage_fault_injection` | 8 | ‚úÖ |

### CI Gate

`scripts/ci_storage_gate.sh` ‚Äî 6 gates: hardening tests, full suite, SST verify, WAL corruption resilience, memory budget, clippy

---

## Distributed Hardening ‚úÖ

**Status**: Complete  
**Goal**: Production-grade distributed coordination ‚Äî epoch fencing, leader leases, shard migration, cross-shard throttling, unified supervisor.

### Deliverables (6 modules, 62 tests)

| Module | Location | Tests | Status |
|--------|----------|------:|--------|
| Global epoch/fencing token (`EpochGuard`, `WriteToken`) | `falcon_cluster::epoch` | 13 | ‚úÖ |
| Raft-managed cluster state machine | `falcon_cluster::consistent_state` | 8 | ‚úÖ |
| Quorum/lease-driven leader authority | `falcon_cluster::leader_lease` | 10 | ‚úÖ |
| Shard migration state machine (5-phase) | `falcon_cluster::migration` | 10 | ‚úÖ |
| Cross-shard txn throttling (Queue/Reject) | `falcon_cluster::cross_shard_throttle` | 7 | ‚úÖ |
| Unified control-plane supervisor | `falcon_cluster::supervisor` | 9 | ‚úÖ |

### Observability

8 new Prometheus metric functions: `record_distributed_metrics`, `record_shard_replication_lag`, `record_epoch_fence_event`, `record_lease_metrics`, `record_migration_metrics`, `record_cross_shard_throttle_metrics`, `record_supervisor_metrics`

### CI Gate

`scripts/ci_distributed_chaos.sh` ‚Äî leader kill, epoch fencing, consistent state recovery, migration interrupt, supervisor degradation

---

## FalconDB Native Protocol ‚úÖ

**Status**: Complete  
**Goal**: High-performance binary protocol for Java/JVM clients ‚Äî replacing PG wire protocol overhead with a purpose-built framing, handshake, compression, and HA-aware failover.

### Rust Crates

| Crate | Modules | Tests | Status |
|-------|---------|------:|--------|
| `falcon_protocol_native` | `types`, `codec`, `compress`, `error` | 39 | ‚úÖ |
| `falcon_native_server` | `server`, `session`, `executor_bridge`, `config`, `error`, `nonce` | 28 | ‚úÖ |

### Protocol Features

| Feature | Status |
|---------|--------|
| Binary framing (LE, 5-byte header, 64 MiB max) | ‚úÖ |
| Handshake + version negotiation (major.minor) | ‚úÖ |
| Feature flags (7 flags: compression, batch, pipeline, epoch, TLS, binary params) | ‚úÖ |
| Password authentication with nonce anti-replay | ‚úÖ |
| Query/Response with binary row encoding + null bitmap | ‚úÖ |
| Batch ingest (columnar rows + per-row error reporting) | ‚úÖ |
| Ping/Pong keepalive | ‚úÖ |
| Graceful disconnect | ‚úÖ |
| LZ4-style compression (negotiated via feature flags) | ‚úÖ |
| Epoch fencing (per-request epoch check) | ‚úÖ |
| StartTLS upgrade (message types defined) | ‚úÖ |

### Java JDBC Driver (`clients/falcondb-jdbc/`)

| Component | Status |
|-----------|--------|
| `FalconDriver` ‚Äî URL parsing, SPI registration, pgjdbc fallback | ‚úÖ |
| `FalconConnection` ‚Äî session management, auto-commit, isValid(ping) | ‚úÖ |
| `FalconStatement` / `FalconPreparedStatement` ‚Äî client-side bind, batch | ‚úÖ |
| `FalconResultSet` / `FalconResultSetMetaData` ‚Äî forward-only, all getter types | ‚úÖ |
| `FalconDataSource` ‚Äî HikariCP-compatible properties | ‚úÖ |
| `ClusterTopologyProvider` ‚Äî seed nodes, primary tracking, stale detection | ‚úÖ |
| `PrimaryResolver` ‚Äî TTL-cached primary resolution | ‚úÖ |
| `FailoverRetryPolicy` ‚Äî configurable retries, exponential backoff | ‚úÖ |
| `FailoverConnection` ‚Äî auto-reconnect on FENCED_EPOCH / NOT_LEADER | ‚úÖ |

### Documentation & Artifacts

- `docs/native_protocol.md` ‚Äî Full protocol specification
- `docs/native_protocol_compat.md` ‚Äî Version negotiation and feature flags
- `clients/falcondb-jdbc/COMPAT_MATRIX.md` ‚Äî JDBC driver compatibility matrix
- `tools/native-proto-spec/vectors/golden_vectors.json` ‚Äî 23 cross-language test vectors

### CI Gate Scripts

- `scripts/ci_native_jdbc_smoke.sh` ‚Äî Rust + Java compile, clippy, test
- `scripts/ci_native_perf_gate.sh` ‚Äî Release build + protocol performance regression
- `scripts/ci_native_failover_under_load.sh` ‚Äî Epoch fencing + failover bench

### Verification

```bash
cargo test -p falcon_protocol_native  # 39 tests
cargo test -p falcon_native_server    # 28 tests
cargo test --workspace                # 2,239 pass, 0 failures
```

---

## Invariants (Enforced Across All Versions from v0.4.x)

These invariants must never be violated once established:

1. **No silent data loss**: A committed transaction (quorum-ack) must survive any single-node failure.
2. **No process panic from user input**: Any malformed SQL, protocol message, or client behavior must return an error, never panic.
3. **No unbounded resource growth**: Memory, WAL backlog, and connection count must be bounded by configured limits.
4. **No stuck in-doubt transactions**: All in-doubt transactions must converge within the configured resolver timeout.
5. **No silent error swallowing**: Every `Err(_)` in a core path must be logged with context or returned to the caller.
6. **Retryable errors must be retryable**: Any error classified `Retryable` must be safe to retry without side effects.
7. **Circuit breakers must isolate**: A failing shard must not cause cascading failures to healthy shards.
